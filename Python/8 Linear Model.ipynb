{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "305db47b-47eb-4a77-aec1-261e29a63b41",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([11])\n"
     ]
    }
   ],
   "source": [
    "import torch \n",
    "t_c = [0.5,  14.0, 15.0, 28.0, 11.0,  8.0,  3.0, -4.0,  6.0, 13.0, 21.0]\n",
    "t_u = [35.7, 55.9, 58.2, 81.9, 56.3, 48.9, 33.9, 21.8, 48.4, 60.4, 68.4]\n",
    "t_c = torch.tensor(t_c)\n",
    "t_u = torch.tensor(t_u)\n",
    "print(t_c.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a3c2c6fd-04d6-4710-8138-09163cd81fed",
   "metadata": {},
   "outputs": [],
   "source": [
    "def model (t_u, w,b):\n",
    "    return w*t_u + b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "62fb0a04-b1fb-429a-a5ef-16d8b1f2fc8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_fn (t_p,t_c):\n",
    "    return torch.mean((t_p - t_c)**2) # ** power of 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "90dda78a-c1af-4a1f-858f-95065e197fb1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(1763.8848)"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w = torch.ones(())\n",
    "b = torch.zeros(())\n",
    "t_p = model(t_u,w,b)\n",
    "loss_fn(t_p,t_c)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba67f6e5-75e8-438b-81b4-50e104a22631",
   "metadata": {},
   "source": [
    "Changing the value of weight w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "80486f40-d0db-4712-8436-9d055f8ada64",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(-44.1730)"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "delta = 0.1\n",
    "loss_rate_of_change_w = \\\n",
    "    (loss_fn(model(t_u, w + delta, b), t_c) - \n",
    "     loss_fn(model(t_u, w - delta, b), t_c)) / (2.0 * delta)\n",
    "#Applying a change to w that is proportional to the rate ofchange of the loss is a good idea\n",
    "learning_rate = 1e-2\n",
    "#the  change  is  positive,  we  need  to decrease w\n",
    "w=w-learning_rate * loss_rate_of_change_w\n",
    "w"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4b10213-6ea8-4943-89f7-28befd3e7dcc",
   "metadata": {},
   "source": [
    "Changing the value of weight w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "61765b7b-0ab4-4c53-912e-8aed9dbcfe20",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(46.)"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss_rate_of_change_b = \\\n",
    "    (loss_fn(model(t_u, w, b + delta), t_c) - \n",
    "     loss_fn(model(t_u, w, b - delta), t_c)) / (2.0 * delta)\n",
    "b=b-learning_rate * loss_rate_of_change_b\n",
    "b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "d9c322fe-4a99-4634-b3ff-aea2202b8072",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(-44.1730)"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "1-learning_rate * loss_rate_of_change_w"
   ]
  },
  {
   "cell_type": "raw",
   "id": "a1a9020f-71cc-4d5c-a5c4-cd800575e4ed",
   "metadata": {},
   "source": [
    "COMPUTING THE DERIVATIVES \n",
    "#Goal: making the neighborhood (+w) infinitesimally small\n",
    "In  order  to  compute  the  derivative  of  the  loss  with  respect  to  a  parameter,  we  can apply  the  chain  rule  and  compute  the  derivative  of  the  loss  with  respect  to  its  input (which is the output of the model), times the derivative of the model with respect to the parameter:\n",
    "d loss_fn/dw=(dloss_fn / d t_p) * (d t_p/dw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "94e7f8e9-d168-4b1e-9ea9-a718f361fd90",
   "metadata": {},
   "outputs": [],
   "source": [
    "def dloss_fn (t_p,t_c):\n",
    "    return (2*(t_p - t_c)/t_p.size(0))\n",
    "def dmodel_dw(t_u, w, b):\n",
    "    return t_u\n",
    "def dmodel_db(t_u, w, b):\n",
    "    return 1.0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2b3ddbd-1f83-43ba-b8c8-0d258f7d67be",
   "metadata": {},
   "source": [
    "DEFINING THE GRADIENT FUNCTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "18e6945a-a7e0-49d3-a1ee-e408be05415c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Putting all of this together, the function returning the gradient of the loss with respect to w and b is\n",
    "def grad_fn(t_u,t_c,t_p,w,b):\n",
    "    w_ = dloss_fn(t_p,t_c)*dmodel_dw(t_u,w,b)\n",
    "    b_ = dloss_fn(t_p,t_c)*dmodel_db(t_u,w,b)\n",
    "    return torch.stack([w_.sum(),b_.sum()])\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "92352c9a-dcfe-489a-aeb0-039600d79a6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def training_loop(n_epochs, learning_rate, params, t_u, t_c):\n",
    "    for epoch in range (1, n_epochs+1):\n",
    "        w,b =params\n",
    "        t_p = model(t_u,w,b) #forward pass\n",
    "        loss = loss_fn(t_p,t_c)\n",
    "        grad = grad_fn(t_u,t_c,t_p,w,b) #Backward pass\n",
    "        params = params - learning_rate*grad;\n",
    "        print('*************Epoch: %d,\\nLoss: %f' % (epoch, float(loss)))   \n",
    "        print('params: ', params)\n",
    "        print('grad: ',grad,'\\n')\n",
    "    return params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "3f532a32-1cd6-404c-8e32-7c4e5958dbd0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*************Epoch: 1,\n",
      "Loss: 1763.884766\n",
      "params:  tensor([-44.1730,  -0.8260])\n",
      "grad:  tensor([4517.2964,   82.6000]) \n",
      "\n",
      "*************Epoch: 2,\n",
      "Loss: 5802484.500000\n",
      "params:  tensor([2568.4011,   45.1637])\n",
      "grad:  tensor([-261257.4062,   -4598.9702]) \n",
      "\n",
      "*************Epoch: 3,\n",
      "Loss: 19408029696.000000\n",
      "params:  tensor([-148527.7344,   -2616.3931])\n",
      "grad:  tensor([15109614.0000,   266155.6875]) \n",
      "\n",
      "*************Epoch: 4,\n",
      "Loss: 64915905708032.000000\n",
      "params:  tensor([8589999.0000,  151310.8906])\n",
      "grad:  tensor([-8.7385e+08, -1.5393e+07]) \n",
      "\n",
      "*************Epoch: 5,\n",
      "Loss: 217130525461053440.000000\n",
      "params:  tensor([-4.9680e+08, -8.7510e+06])\n",
      "grad:  tensor([5.0539e+10, 8.9023e+08]) \n",
      "\n",
      "*************Epoch: 6,\n",
      "Loss: 726257583152928129024.000000\n",
      "params:  tensor([2.8732e+10, 5.0610e+08])\n",
      "grad:  tensor([-2.9229e+12, -5.1486e+10]) \n",
      "\n",
      "*************Epoch: 7,\n",
      "Loss: 2429183416467662896627712.000000\n",
      "params:  tensor([-1.6617e+12, -2.9270e+10])\n",
      "grad:  tensor([1.6904e+14, 2.9776e+12]) \n",
      "\n",
      "*************Epoch: 8,\n",
      "Loss: 8125122549611731432050262016.000000\n",
      "params:  tensor([9.6102e+13, 1.6928e+12])\n",
      "grad:  tensor([-9.7764e+15, -1.7221e+14]) \n",
      "\n",
      "*************Epoch: 9,\n",
      "Loss: 27176882120842590626938030653440.000000\n",
      "params:  tensor([-5.5580e+15, -9.7903e+13])\n",
      "grad:  tensor([5.6541e+17, 9.9596e+15]) \n",
      "\n",
      "*************Epoch: 10,\n",
      "Loss: 90901105189019073810297959556841472.000000\n",
      "params:  tensor([3.2144e+17, 5.6621e+15])\n",
      "grad:  tensor([-3.2700e+19, -5.7600e+17]) \n",
      "\n",
      "*************Epoch: 11,\n",
      "Loss: inf\n",
      "params:  tensor([-1.8590e+19, -3.2746e+17])\n",
      "grad:  tensor([1.8912e+21, 3.3313e+19]) \n",
      "\n",
      "*************Epoch: 12,\n",
      "Loss: inf\n",
      "params:  tensor([1.0752e+21, 1.8939e+19])\n",
      "grad:  tensor([-1.0937e+23, -1.9266e+21]) \n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([1.0752e+21, 1.8939e+19])"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#overfitting\n",
    "training_loop(\n",
    "    n_epochs = 12,\n",
    "    learning_rate = 1e-2,\n",
    "    params = torch.tensor([1.0, 0.0]),\n",
    "    t_u = t_u,\n",
    "    t_c = t_c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "64dc2d82-e47a-4dd3-9487-fb3d3bee4072",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*************Epoch: 1,\n",
      "Loss: 1763.884766\n",
      "params:  tensor([ 0.5483, -0.0083])\n",
      "grad:  tensor([4517.2964,   82.6000]) \n",
      "\n",
      "*************Epoch: 2,\n",
      "Loss: 323.090515\n",
      "params:  tensor([ 0.3623, -0.0118])\n",
      "grad:  tensor([1859.5493,   35.7843]) \n",
      "\n",
      "*************Epoch: 3,\n",
      "Loss: 78.929634\n",
      "params:  tensor([ 0.2858, -0.0135])\n",
      "grad:  tensor([765.4666,  16.5122]) \n",
      "\n",
      "*************Epoch: 4,\n",
      "Loss: 37.552845\n",
      "params:  tensor([ 0.2543, -0.0143])\n",
      "grad:  tensor([315.0790,   8.5787]) \n",
      "\n",
      "*************Epoch: 5,\n",
      "Loss: 30.540283\n",
      "params:  tensor([ 0.2413, -0.0149])\n",
      "grad:  tensor([129.6733,   5.3127]) \n",
      "\n",
      "*************Epoch: 6,\n",
      "Loss: 29.351154\n",
      "params:  tensor([ 0.2360, -0.0153])\n",
      "grad:  tensor([53.3495,  3.9682]) \n",
      "\n",
      "*************Epoch: 7,\n",
      "Loss: 29.148884\n",
      "params:  tensor([ 0.2338, -0.0156])\n",
      "grad:  tensor([21.9304,  3.4148]) \n",
      "\n",
      "*************Epoch: 8,\n",
      "Loss: 29.113848\n",
      "params:  tensor([ 0.2329, -0.0159])\n",
      "grad:  tensor([8.9964, 3.1869]) \n",
      "\n",
      "*************Epoch: 9,\n",
      "Loss: 29.107145\n",
      "params:  tensor([ 0.2325, -0.0162])\n",
      "grad:  tensor([3.6721, 3.0930]) \n",
      "\n",
      "*************Epoch: 10,\n",
      "Loss: 29.105247\n",
      "params:  tensor([ 0.2324, -0.0166])\n",
      "grad:  tensor([1.4803, 3.0544]) \n",
      "\n",
      "*************Epoch: 11,\n",
      "Loss: 29.104168\n",
      "params:  tensor([ 0.2323, -0.0169])\n",
      "grad:  tensor([0.5781, 3.0384]) \n",
      "\n",
      "*************Epoch: 12,\n",
      "Loss: 29.103222\n",
      "params:  tensor([ 0.2323, -0.0172])\n",
      "grad:  tensor([0.2066, 3.0318]) \n",
      "\n",
      "*************Epoch: 13,\n",
      "Loss: 29.102295\n",
      "params:  tensor([ 0.2323, -0.0175])\n",
      "grad:  tensor([0.0537, 3.0291]) \n",
      "\n",
      "*************Epoch: 14,\n",
      "Loss: 29.101379\n",
      "params:  tensor([ 0.2323, -0.0178])\n",
      "grad:  tensor([-0.0093,  3.0279]) \n",
      "\n",
      "*************Epoch: 15,\n",
      "Loss: 29.100466\n",
      "params:  tensor([ 0.2323, -0.0181])\n",
      "grad:  tensor([-0.0353,  3.0274]) \n",
      "\n",
      "*************Epoch: 16,\n",
      "Loss: 29.099548\n",
      "params:  tensor([ 0.2323, -0.0184])\n",
      "grad:  tensor([-0.0459,  3.0272]) \n",
      "\n",
      "*************Epoch: 17,\n",
      "Loss: 29.098631\n",
      "params:  tensor([ 0.2323, -0.0187])\n",
      "grad:  tensor([-0.0502,  3.0270]) \n",
      "\n",
      "*************Epoch: 18,\n",
      "Loss: 29.097717\n",
      "params:  tensor([ 0.2323, -0.0190])\n",
      "grad:  tensor([-0.0520,  3.0270]) \n",
      "\n",
      "*************Epoch: 19,\n",
      "Loss: 29.096796\n",
      "params:  tensor([ 0.2323, -0.0193])\n",
      "grad:  tensor([-0.0528,  3.0269]) \n",
      "\n",
      "*************Epoch: 20,\n",
      "Loss: 29.095881\n",
      "params:  tensor([ 0.2323, -0.0196])\n",
      "grad:  tensor([-0.0531,  3.0268]) \n",
      "\n",
      "*************Epoch: 21,\n",
      "Loss: 29.094959\n",
      "params:  tensor([ 0.2323, -0.0199])\n",
      "grad:  tensor([-0.0533,  3.0268]) \n",
      "\n",
      "*************Epoch: 22,\n",
      "Loss: 29.094049\n",
      "params:  tensor([ 0.2323, -0.0202])\n",
      "grad:  tensor([-0.0533,  3.0267]) \n",
      "\n",
      "*************Epoch: 23,\n",
      "Loss: 29.093134\n",
      "params:  tensor([ 0.2323, -0.0205])\n",
      "grad:  tensor([-0.0533,  3.0267]) \n",
      "\n",
      "*************Epoch: 24,\n",
      "Loss: 29.092216\n",
      "params:  tensor([ 0.2323, -0.0208])\n",
      "grad:  tensor([-0.0533,  3.0266]) \n",
      "\n",
      "*************Epoch: 25,\n",
      "Loss: 29.091301\n",
      "params:  tensor([ 0.2323, -0.0211])\n",
      "grad:  tensor([-0.0533,  3.0266]) \n",
      "\n",
      "*************Epoch: 26,\n",
      "Loss: 29.090385\n",
      "params:  tensor([ 0.2323, -0.0214])\n",
      "grad:  tensor([-0.0533,  3.0265]) \n",
      "\n",
      "*************Epoch: 27,\n",
      "Loss: 29.089464\n",
      "params:  tensor([ 0.2323, -0.0217])\n",
      "grad:  tensor([-0.0533,  3.0265]) \n",
      "\n",
      "*************Epoch: 28,\n",
      "Loss: 29.088551\n",
      "params:  tensor([ 0.2323, -0.0220])\n",
      "grad:  tensor([-0.0532,  3.0264]) \n",
      "\n",
      "*************Epoch: 29,\n",
      "Loss: 29.087635\n",
      "params:  tensor([ 0.2323, -0.0223])\n",
      "grad:  tensor([-0.0533,  3.0264]) \n",
      "\n",
      "*************Epoch: 30,\n",
      "Loss: 29.086714\n",
      "params:  tensor([ 0.2323, -0.0226])\n",
      "grad:  tensor([-0.0533,  3.0263]) \n",
      "\n",
      "*************Epoch: 31,\n",
      "Loss: 29.085804\n",
      "params:  tensor([ 0.2324, -0.0229])\n",
      "grad:  tensor([-0.0532,  3.0262]) \n",
      "\n",
      "*************Epoch: 32,\n",
      "Loss: 29.084888\n",
      "params:  tensor([ 0.2324, -0.0232])\n",
      "grad:  tensor([-0.0533,  3.0262]) \n",
      "\n",
      "*************Epoch: 33,\n",
      "Loss: 29.083967\n",
      "params:  tensor([ 0.2324, -0.0235])\n",
      "grad:  tensor([-0.0533,  3.0261]) \n",
      "\n",
      "*************Epoch: 34,\n",
      "Loss: 29.083057\n",
      "params:  tensor([ 0.2324, -0.0238])\n",
      "grad:  tensor([-0.0533,  3.0261]) \n",
      "\n",
      "*************Epoch: 35,\n",
      "Loss: 29.082142\n",
      "params:  tensor([ 0.2324, -0.0241])\n",
      "grad:  tensor([-0.0532,  3.0260]) \n",
      "\n",
      "*************Epoch: 36,\n",
      "Loss: 29.081221\n",
      "params:  tensor([ 0.2324, -0.0244])\n",
      "grad:  tensor([-0.0533,  3.0260]) \n",
      "\n",
      "*************Epoch: 37,\n",
      "Loss: 29.080309\n",
      "params:  tensor([ 0.2324, -0.0247])\n",
      "grad:  tensor([-0.0533,  3.0259]) \n",
      "\n",
      "*************Epoch: 38,\n",
      "Loss: 29.079390\n",
      "params:  tensor([ 0.2324, -0.0250])\n",
      "grad:  tensor([-0.0532,  3.0259]) \n",
      "\n",
      "*************Epoch: 39,\n",
      "Loss: 29.078474\n",
      "params:  tensor([ 0.2324, -0.0253])\n",
      "grad:  tensor([-0.0533,  3.0258]) \n",
      "\n",
      "*************Epoch: 40,\n",
      "Loss: 29.077562\n",
      "params:  tensor([ 0.2324, -0.0256])\n",
      "grad:  tensor([-0.0533,  3.0258]) \n",
      "\n",
      "*************Epoch: 41,\n",
      "Loss: 29.076649\n",
      "params:  tensor([ 0.2324, -0.0259])\n",
      "grad:  tensor([-0.0533,  3.0257]) \n",
      "\n",
      "*************Epoch: 42,\n",
      "Loss: 29.075731\n",
      "params:  tensor([ 0.2324, -0.0262])\n",
      "grad:  tensor([-0.0532,  3.0257]) \n",
      "\n",
      "*************Epoch: 43,\n",
      "Loss: 29.074812\n",
      "params:  tensor([ 0.2324, -0.0265])\n",
      "grad:  tensor([-0.0533,  3.0256]) \n",
      "\n",
      "*************Epoch: 44,\n",
      "Loss: 29.073895\n",
      "params:  tensor([ 0.2324, -0.0268])\n",
      "grad:  tensor([-0.0533,  3.0256]) \n",
      "\n",
      "*************Epoch: 45,\n",
      "Loss: 29.072981\n",
      "params:  tensor([ 0.2324, -0.0271])\n",
      "grad:  tensor([-0.0533,  3.0255]) \n",
      "\n",
      "*************Epoch: 46,\n",
      "Loss: 29.072069\n",
      "params:  tensor([ 0.2324, -0.0274])\n",
      "grad:  tensor([-0.0533,  3.0254]) \n",
      "\n",
      "*************Epoch: 47,\n",
      "Loss: 29.071148\n",
      "params:  tensor([ 0.2324, -0.0277])\n",
      "grad:  tensor([-0.0533,  3.0254]) \n",
      "\n",
      "*************Epoch: 48,\n",
      "Loss: 29.070234\n",
      "params:  tensor([ 0.2324, -0.0281])\n",
      "grad:  tensor([-0.0533,  3.0253]) \n",
      "\n",
      "*************Epoch: 49,\n",
      "Loss: 29.069323\n",
      "params:  tensor([ 0.2325, -0.0284])\n",
      "grad:  tensor([-0.0533,  3.0253]) \n",
      "\n",
      "*************Epoch: 50,\n",
      "Loss: 29.068401\n",
      "params:  tensor([ 0.2325, -0.0287])\n",
      "grad:  tensor([-0.0532,  3.0252]) \n",
      "\n",
      "*************Epoch: 51,\n",
      "Loss: 29.067486\n",
      "params:  tensor([ 0.2325, -0.0290])\n",
      "grad:  tensor([-0.0533,  3.0252]) \n",
      "\n",
      "*************Epoch: 52,\n",
      "Loss: 29.066566\n",
      "params:  tensor([ 0.2325, -0.0293])\n",
      "grad:  tensor([-0.0533,  3.0251]) \n",
      "\n",
      "*************Epoch: 53,\n",
      "Loss: 29.065657\n",
      "params:  tensor([ 0.2325, -0.0296])\n",
      "grad:  tensor([-0.0533,  3.0251]) \n",
      "\n",
      "*************Epoch: 54,\n",
      "Loss: 29.064741\n",
      "params:  tensor([ 0.2325, -0.0299])\n",
      "grad:  tensor([-0.0533,  3.0250]) \n",
      "\n",
      "*************Epoch: 55,\n",
      "Loss: 29.063826\n",
      "params:  tensor([ 0.2325, -0.0302])\n",
      "grad:  tensor([-0.0532,  3.0250]) \n",
      "\n",
      "*************Epoch: 56,\n",
      "Loss: 29.062910\n",
      "params:  tensor([ 0.2325, -0.0305])\n",
      "grad:  tensor([-0.0533,  3.0249]) \n",
      "\n",
      "*************Epoch: 57,\n",
      "Loss: 29.061995\n",
      "params:  tensor([ 0.2325, -0.0308])\n",
      "grad:  tensor([-0.0532,  3.0249]) \n",
      "\n",
      "*************Epoch: 58,\n",
      "Loss: 29.061079\n",
      "params:  tensor([ 0.2325, -0.0311])\n",
      "grad:  tensor([-0.0533,  3.0248]) \n",
      "\n",
      "*************Epoch: 59,\n",
      "Loss: 29.060169\n",
      "params:  tensor([ 0.2325, -0.0314])\n",
      "grad:  tensor([-0.0533,  3.0248]) \n",
      "\n",
      "*************Epoch: 60,\n",
      "Loss: 29.059248\n",
      "params:  tensor([ 0.2325, -0.0317])\n",
      "grad:  tensor([-0.0533,  3.0247]) \n",
      "\n",
      "*************Epoch: 61,\n",
      "Loss: 29.058336\n",
      "params:  tensor([ 0.2325, -0.0320])\n",
      "grad:  tensor([-0.0533,  3.0247]) \n",
      "\n",
      "*************Epoch: 62,\n",
      "Loss: 29.057415\n",
      "params:  tensor([ 0.2325, -0.0323])\n",
      "grad:  tensor([-0.0534,  3.0246]) \n",
      "\n",
      "*************Epoch: 63,\n",
      "Loss: 29.056507\n",
      "params:  tensor([ 0.2325, -0.0326])\n",
      "grad:  tensor([-0.0533,  3.0245]) \n",
      "\n",
      "*************Epoch: 64,\n",
      "Loss: 29.055586\n",
      "params:  tensor([ 0.2325, -0.0329])\n",
      "grad:  tensor([-0.0532,  3.0245]) \n",
      "\n",
      "*************Epoch: 65,\n",
      "Loss: 29.054674\n",
      "params:  tensor([ 0.2325, -0.0332])\n",
      "grad:  tensor([-0.0533,  3.0244]) \n",
      "\n",
      "*************Epoch: 66,\n",
      "Loss: 29.053761\n",
      "params:  tensor([ 0.2325, -0.0335])\n",
      "grad:  tensor([-0.0533,  3.0244]) \n",
      "\n",
      "*************Epoch: 67,\n",
      "Loss: 29.052843\n",
      "params:  tensor([ 0.2325, -0.0338])\n",
      "grad:  tensor([-0.0533,  3.0243]) \n",
      "\n",
      "*************Epoch: 68,\n",
      "Loss: 29.051929\n",
      "params:  tensor([ 0.2326, -0.0341])\n",
      "grad:  tensor([-0.0532,  3.0243]) \n",
      "\n",
      "*************Epoch: 69,\n",
      "Loss: 29.051012\n",
      "params:  tensor([ 0.2326, -0.0344])\n",
      "grad:  tensor([-0.0533,  3.0242]) \n",
      "\n",
      "*************Epoch: 70,\n",
      "Loss: 29.050098\n",
      "params:  tensor([ 0.2326, -0.0347])\n",
      "grad:  tensor([-0.0532,  3.0242]) \n",
      "\n",
      "*************Epoch: 71,\n",
      "Loss: 29.049183\n",
      "params:  tensor([ 0.2326, -0.0350])\n",
      "grad:  tensor([-0.0533,  3.0241]) \n",
      "\n",
      "*************Epoch: 72,\n",
      "Loss: 29.048273\n",
      "params:  tensor([ 0.2326, -0.0353])\n",
      "grad:  tensor([-0.0533,  3.0241]) \n",
      "\n",
      "*************Epoch: 73,\n",
      "Loss: 29.047350\n",
      "params:  tensor([ 0.2326, -0.0356])\n",
      "grad:  tensor([-0.0532,  3.0240]) \n",
      "\n",
      "*************Epoch: 74,\n",
      "Loss: 29.046442\n",
      "params:  tensor([ 0.2326, -0.0359])\n",
      "grad:  tensor([-0.0533,  3.0240]) \n",
      "\n",
      "*************Epoch: 75,\n",
      "Loss: 29.045530\n",
      "params:  tensor([ 0.2326, -0.0362])\n",
      "grad:  tensor([-0.0532,  3.0239]) \n",
      "\n",
      "*************Epoch: 76,\n",
      "Loss: 29.044611\n",
      "params:  tensor([ 0.2326, -0.0365])\n",
      "grad:  tensor([-0.0533,  3.0239]) \n",
      "\n",
      "*************Epoch: 77,\n",
      "Loss: 29.043699\n",
      "params:  tensor([ 0.2326, -0.0368])\n",
      "grad:  tensor([-0.0533,  3.0238]) \n",
      "\n",
      "*************Epoch: 78,\n",
      "Loss: 29.042784\n",
      "params:  tensor([ 0.2326, -0.0371])\n",
      "grad:  tensor([-0.0533,  3.0238]) \n",
      "\n",
      "*************Epoch: 79,\n",
      "Loss: 29.041870\n",
      "params:  tensor([ 0.2326, -0.0374])\n",
      "grad:  tensor([-0.0533,  3.0237]) \n",
      "\n",
      "*************Epoch: 80,\n",
      "Loss: 29.040955\n",
      "params:  tensor([ 0.2326, -0.0377])\n",
      "grad:  tensor([-0.0532,  3.0236]) \n",
      "\n",
      "*************Epoch: 81,\n",
      "Loss: 29.040039\n",
      "params:  tensor([ 0.2326, -0.0380])\n",
      "grad:  tensor([-0.0534,  3.0236]) \n",
      "\n",
      "*************Epoch: 82,\n",
      "Loss: 29.039122\n",
      "params:  tensor([ 0.2326, -0.0383])\n",
      "grad:  tensor([-0.0533,  3.0235]) \n",
      "\n",
      "*************Epoch: 83,\n",
      "Loss: 29.038210\n",
      "params:  tensor([ 0.2326, -0.0386])\n",
      "grad:  tensor([-0.0532,  3.0235]) \n",
      "\n",
      "*************Epoch: 84,\n",
      "Loss: 29.037294\n",
      "params:  tensor([ 0.2326, -0.0389])\n",
      "grad:  tensor([-0.0533,  3.0234]) \n",
      "\n",
      "*************Epoch: 85,\n",
      "Loss: 29.036379\n",
      "params:  tensor([ 0.2326, -0.0392])\n",
      "grad:  tensor([-0.0533,  3.0234]) \n",
      "\n",
      "*************Epoch: 86,\n",
      "Loss: 29.035463\n",
      "params:  tensor([ 0.2326, -0.0395])\n",
      "grad:  tensor([-0.0532,  3.0233]) \n",
      "\n",
      "*************Epoch: 87,\n",
      "Loss: 29.034554\n",
      "params:  tensor([ 0.2327, -0.0398])\n",
      "grad:  tensor([-0.0533,  3.0233]) \n",
      "\n",
      "*************Epoch: 88,\n",
      "Loss: 29.033636\n",
      "params:  tensor([ 0.2327, -0.0401])\n",
      "grad:  tensor([-0.0532,  3.0232]) \n",
      "\n",
      "*************Epoch: 89,\n",
      "Loss: 29.032722\n",
      "params:  tensor([ 0.2327, -0.0405])\n",
      "grad:  tensor([-0.0533,  3.0232]) \n",
      "\n",
      "*************Epoch: 90,\n",
      "Loss: 29.031811\n",
      "params:  tensor([ 0.2327, -0.0408])\n",
      "grad:  tensor([-0.0533,  3.0231]) \n",
      "\n",
      "*************Epoch: 91,\n",
      "Loss: 29.030895\n",
      "params:  tensor([ 0.2327, -0.0411])\n",
      "grad:  tensor([-0.0532,  3.0231]) \n",
      "\n",
      "*************Epoch: 92,\n",
      "Loss: 29.029976\n",
      "params:  tensor([ 0.2327, -0.0414])\n",
      "grad:  tensor([-0.0532,  3.0230]) \n",
      "\n",
      "*************Epoch: 93,\n",
      "Loss: 29.029066\n",
      "params:  tensor([ 0.2327, -0.0417])\n",
      "grad:  tensor([-0.0533,  3.0230]) \n",
      "\n",
      "*************Epoch: 94,\n",
      "Loss: 29.028151\n",
      "params:  tensor([ 0.2327, -0.0420])\n",
      "grad:  tensor([-0.0532,  3.0229]) \n",
      "\n",
      "*************Epoch: 95,\n",
      "Loss: 29.027235\n",
      "params:  tensor([ 0.2327, -0.0423])\n",
      "grad:  tensor([-0.0533,  3.0229]) \n",
      "\n",
      "*************Epoch: 96,\n",
      "Loss: 29.026323\n",
      "params:  tensor([ 0.2327, -0.0426])\n",
      "grad:  tensor([-0.0533,  3.0228]) \n",
      "\n",
      "*************Epoch: 97,\n",
      "Loss: 29.025410\n",
      "params:  tensor([ 0.2327, -0.0429])\n",
      "grad:  tensor([-0.0532,  3.0227]) \n",
      "\n",
      "*************Epoch: 98,\n",
      "Loss: 29.024492\n",
      "params:  tensor([ 0.2327, -0.0432])\n",
      "grad:  tensor([-0.0532,  3.0227]) \n",
      "\n",
      "*************Epoch: 99,\n",
      "Loss: 29.023582\n",
      "params:  tensor([ 0.2327, -0.0435])\n",
      "grad:  tensor([-0.0533,  3.0226]) \n",
      "\n",
      "*************Epoch: 100,\n",
      "Loss: 29.022667\n",
      "params:  tensor([ 0.2327, -0.0438])\n",
      "grad:  tensor([-0.0532,  3.0226]) \n",
      "\n",
      "*************Epoch: 101,\n",
      "Loss: 29.021753\n",
      "params:  tensor([ 0.2327, -0.0441])\n",
      "grad:  tensor([-0.0532,  3.0225]) \n",
      "\n",
      "*************Epoch: 102,\n",
      "Loss: 29.020842\n",
      "params:  tensor([ 0.2327, -0.0444])\n",
      "grad:  tensor([-0.0532,  3.0225]) \n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([ 0.2327, -0.0444])"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#hyperparameter  tuning\n",
    "learning_rate = 1e-4\n",
    "training_loop(\n",
    "    n_epochs = 102,\n",
    "    learning_rate=learning_rate,\n",
    "    params = torch.tensor([1.0, 0.0]),\n",
    "    t_u = t_u,\n",
    "    t_c = t_c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb61ac04-cb68-4384-a29e-d02a15541d18",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
